[
 {lager, [
   {log_root, "log/"},
   {colored, true},
   {colors, [
        {debug,     "\e[0;33m" },
        {info,      "\e[0;32m" },
        {notice,    "\e[1;32m" },
        {warning,   "\e[0;35m" },
        {error,     "\e[1;31m" },
        {critical,  "\e[1;31m" },
        {alert,     "\e[1;31m" },
        {emergency, "\e[1;31m" }
   ]},
   {handlers, [
     %%{lager_console_backend, [{level, debug}]},
     {lager_file_backend, [{file, "error.log"}, {level, error}, {size, 10485760}, {date, "$D0"}, {count, 10}]},
     {lager_file_backend, [{file, "console.log"}, {level, info}, {size, 10485760}, {date, "$D0"}, {count, 10}]}
   ]},
   %% {error_logger_hwm, 250}
   {extra_sinks, [
     {req_logs_lager_event, [
         {handlers, [
             %%{lager_console_backend, [
             %%    {level, info},
             %%    {formatter_config, [time, color, " [",severity,"] ", message, "\e[0m\r\n"]}
             %%]},
             {lager_file_backend, [{file, "req.log"}, {level, info}, {size, 10485760}, {date, "$D0"}, {count, 10}]}
         ]}
     ]}
   ]}
 ]},
 {riak_core, [
   {ring_state_dir, "/tmp/egraph-ring"},
   {web_port, 8098},
   {handoff_port, 8099},
   {schema_dirs, ["/tmp/egraph-schema"]}
 ]},
 %% add else folsom_cowboy will listen on 127.0.0.1
 {folsom_cowboy, [
     {ip, {0, 0, 0, 0}},
     {port, 5565}
  ]},
 {egraph, [

   %% ------------- business jobs -----------

   {jobs, [
   ]},

   %% ---------------------------------------

   {circuit_breaker_delta_msec, 5000},

   %% maxt and reset are in millisecond
   %% maxr is number of melts in maxt time window
   {circuit_breakers, [
     {search, [{maxr, 1000}, {maxt, 60000}, {reset, 10000}]}
   ]},

   {opentracing, [
       %% set to false to disable opentracing (via otter)
       %% see otter app setting for information
       {enable, false}
   ]},

   %% ------------- http server settings ----

   {http_rest, [
       {log_enabled, true},  %% set it to true to get info in req.log
       {port, 8001},          %% HTTP listen port

       {nr_listeners, 1000},
       {backlog, 1024},
       {max_connections, 50000},
       %% set ssl to false for http:// instead of https://
       %% if ssl is setup then ensure that priv/ssl/cert.pem is the server cert
       %% and priv/ssl/key.pem is the server private key in PEM format.
       {ssl, false},

       %% the maximum number of requests which will be serviced
       %% with the same process (by cowboy) when http client
       %% implements HTTP/1.1 keep-alive
       {max_keepalive, 100},

       {max_read_length, 12582912},  %% 12 * 1024 * 1024 or 12 MB
       {max_read_timeout_msec, 10000}
   ]},

   %% ---------------------------------------

   {caches, [
       %% must match egraph_constants.hrl
       {cache_generic, [
           {enable, true},
           {memory_bytes, 1073741824}, %% 1024*1024*1024 or 1GB
           {segments, 1},
           {ttl_sec, 31536000} %% 365*24*60*60 or 1 year
       ]}
    ]},

   %% --------- index settings --------------

   {enable_reindex, true},
   {reindex_max_shard_per_run, 5},

   %% --- graph index settings --------------
   %% IMPORTANT: The pools mentioned below must exist
   %% within the palma_pools configuration.

   %% Notice that only pool must exist for adding or removing entries
   %% from index tables.
   {index_rw_pool, egraphdbrw_pool},

   %% In case the ro pools are more than one then the application
   %% will do load distribution across the pools and also use them
   %% for fallback.
   {index_ro_pools, [egraphdbro_pool]},

   {index_rw_timeout_msec, 5000},
   {index_ro_timeout_msec, 5000},
   %% ---------------------------------------

   %% --- graph link settings --------------
   %% IMPORTANT: The pools mentioned below must exist
   %% within the palma_pools configuration.

   %% Notice that only pool must exist for adding or removing entries
   %% from index tables.
   {link_rw_pool, egraphdbrw_pool},

   %% In case the ro pools are more than one then the application
   %% will do load distribution across the pools and also use them
   %% for fallback.
   {link_ro_pools, [egraphdbro_pool]},

   {link_rw_timeout_msec, 5000},
   {link_ro_timeout_msec, 5000},
   %% ---------------------------------------

   %% --- graph detail settings -------------
   %% IMPORTANT: The pools mentioned below must exist
   %% within the palma_pools configuration.

   %% Notice that only pool must exist for adding or removing entries
   %% from index tables.
   {detail_rw_pool, egraphdbrw_pool},

   %% In case the ro pools are more than one then the application
   %% will do load distribution across the pools and also use them
   %% for fallback.
   {detail_ro_pools, [egraphdbro_pool]},

   {detail_rw_timeout_msec, 5000},
   {detail_ro_timeout_msec, 5000},
   %% ---------------------------------------

   {palma_pools, [
            {
                egraphdbro_pool,
                1,  %% Total number of pool members
                {
                    mysql_dbro_worker_id,
                    {
                        mysql,
                        start_link,
                        [
                            [
                                {host, "127.0.0.1"}, %% mysql module uses host instead of hostname
                                {database, "egraph_db"},
                                {port, 3306},
                                {user, "egraph_user"}, %% mysql module requires this to be user instead of username
                                {password, "abc123"} ,
                                {keep_alive, 10000},  %% send mysql ping every 10 seconds if not used
                                {connect_timeout, 1000},  %% millisecond connect timeout
                                {query_cache_time, 500}
                            ]
                        ]
                    },
                    {permanent, 5},  %% see palma_supervisor2 (delay of 5 seconds)
                    2000,  %% milli seconds to wait before killing
                    worker,  %% it is a worker (and not supervisor)
                    [mysql]
                },
                10000, %% palma shutdown delay in milliseconds
                #{ min_alive_ratio => 1.0, reconnect_delay => 4000}  %% revolver options
            },
			{
                egraphdbrw_pool,
                1,  %% Total number of pool members
                {
                    mysql_dbrw_worker_id,
                    {
                        mysql,
                        start_link,
                        [
                            [
                                {host, "127.0.0.1"}, %% mysql module uses host instead of hostname
                                {database, "egraph_db"},
                                {port, 3306},
                                {user, "egraph_user"}, %% mysql module requires this to be user instead of username
                                {password, "abc123"} ,
                                {keep_alive, 10000},  %% send mysql ping every 10 seconds if not used
                                {connect_timeout, 1000},  %% millisecond connect timeout
                                {query_cache_time, 500}
                            ]
                        ]
                    },
                    {permanent, 5},  %% see palma_supervisor2 (delay of 5 seconds)
                    2000,  %% milli seconds to wait before killing
                    worker,  %% it is a worker (and not supervisor)
                    [mysql]
                },
                10000, %% palma shutdown delay in milliseconds
                #{ min_alive_ratio => 1.0, reconnect_delay => 4000}  %% revolver options
            }
   ]},

   %%% Monitoring bridge between Folsom and Graphite
   {folsom_graphite, [
       {enabled, false},  %% true when metrics are pushed to Graphite
       {endpoint, {"127.0.0.1", 5555}},
       {buckets, [
            [
                {name, egraph_folsom_core_worker},
                {bucket, "egraph"},
                {dimensions, [{cluster, <<"app">>}, {node, node}]},
                {folsom_table_prefixes, [
                    <<"api.">>,
                    <<"api-n.">>,
                    <<"sm.">>,
                    <<"2x">>,
                    <<"4x">>,
                    <<"5x">>
                ]},
                {interval, 5000}
            ]
       ]}
   ]},

   %% measure system metrics as given below
   {sysmon, [
        {enabled, true},
        {interval, 5000},
        {vm_metrics, [
                {cpu, avg1},
                {cpu, avg5},
                {cpu, avg15},
                {cpu, util},
                {memory, total},
                {memory, processes},
                {memory, ets},
                {memory, binary},
                {memory, atom},
                {run_queue, all},
                {scheduler, util},
                {io, all},
                {disk, all}
        ]}
   ]}
 ]},

 %% let fuse use folsom for metrics
 {fuse, [
   {stats_plugin, fuse_stats_folsom}
 ]},

 %% default configuration taken from otter/otter.app.src
 %% This is unnecessary unless you want to change it here
 %% or in the dynamic function to load configuration
 %% each time
 {otter, [
    {http_client, httpc}, %% ibrowse | httpc
    %% send to openzipkin
    %% {zipkin_collector_uri, "http://127.0.0.1:9411/api/v1/spans"},
    %% use the following (instead of above) if you want to send to Jaeger
    {zipkin_collector_uri, "http://127.0.0.1:9411/api/v1/spans"},
    %% {zipkin_collector_uri, "http://127.0.0.1:14268/api/traces?format=zipkin.thrift"},
    {zipkin_batch_interval_ms, 100},
    {zipkin_tag_host_ip, {127,0,0,1}},
    {zipkin_tag_host_port, 0},
    {zipkin_tag_host_service, "egraph"},
    {zipkin_add_host_tag_to_span, {"lc", ""}},
    {filter_rules, [
        {
            [
                %% prefilter
                {greater, otter_span_duration, 1}
            ],
            [
                %% final filter rules
                {snapshot_count, [long_span], [otter_span_name]}
            ]
        },
        {
            [
                %% prefilter
            ],
            [
                %% final filter rules
                {snapshot_count, [span_processed], [otter_span_name]},
                send_to_zipkin
            ]
        }
    ]}
  ]}
].

